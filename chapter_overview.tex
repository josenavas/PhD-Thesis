\glsresetall

\section{The microbiome and big data}\label{section_bigdata}

Microbiome datasets have expanded rapidly in recent years. Advances in DNA sequencing, as well as the rise of shotgun metagenomics and metabolomics, are producing datasets that exceed the ability of researchers to analyze them on their personal computers. Here we describe what Big Data is in the context of microbiome research, how this data can be transformed into knowledge about microbes and their functions in their environments, and how the knowledge can be applied to move microbiome research forward. In particular, the development of new high-resolution tools to assess strain-level variability (moving away from \gls{otu}), the advent of cloud computing and centralized analysis resources such as Qiita (for sequences) and \gls{gnps} (for mass spectrometry), and better methods for curating and describing “metadata” (contextual information about the sequence or chemical information) are rapidly assisting the use of microbiome data in fields ranging from human health to environmental studies.

\subsection{From cells to bits: what is big data in microbiome research?}

Since the term “microbiome” was coined by Joshua Lederberg in 2001 \cite{Lederberg2001}, the microbiome research field has exploded both in terms of the heterogeneity of the data produced and in the amount of data generated. Early approaches to characterizing the microbiome were based on targeted detection techniques in the laboratory, such as culturing and assays based on the \gls{pcr}, and assessed limited numbers of subjects (on the order of tens) \cite{Brigidi2001}. The introduction of sequencing technologies revolutionized the field, enabling investigators to characterize microbial communities directly from primary samples. Historically, the 16S \gls{rrna} gene, a marker gene that exists in all bacteria and archaea as an essential part of the ribosome, has been targeted for these sequence-based profiling efforts. Its ubiquity among bacteria and archaea and the low cost of the approach has made it the most widely used for microbiome profiling of samples. Similarly, amplification and sequencing of the 18S \gls{rrna} gene and the \gls{its} permit investigators to profile the eukaryotic and fungal communities present in a sample using similar techniques. Since the introduction of \gls{ngs}, technologies have evolved from generating a few hundred thousand reads per run (454 GS) to tens of million reads (Illumina MiSeq) or even a few billion reads per run (Illumina HiSeq) \cite{Goodwin2016}. Benchmarked protocols, such as those used by the \gls{emp} and widely adopted by researchers around the globe, facilitate meta-analyses of unprecedented size-investigators can combine studies, each with hundreds to thousands of samples, into a single large analysis effort.
The precipitous drop in sample processing and sequencing costs associated with new technology development is enabling researchers to move beyond simple taxonomy and abundance-based work to species and strain level profiling as well as descriptions of functional pathways through whole genome shotgun metagenomics sequencing. As a result, researchers are able to ask more critical questions of their samples and are utilizing other technologies, such as detection of small molecules via mass spectrometry, to confirm or refute hypotheses driven by functional pathway and gene abundance information obtained from shotgun sequencing data.

The rate at which these technologies are increasing their data output is faster than our computational power is growing \cite{Wetterstrand2013}, effectively shifting the costs of a research study from the sequencing pipeline to the data analysis pipeline. Additionally, as researchers utilize larger and larger datasets, they are able to design large-scale studies to ask (and answer) complex questions. The metadata associated with samples, therefore, is becoming an increasingly large contributor to microbiome big data and the challenges associated with streamlining data analysis. Standards such as \gls{mimarks} \cite{Yilmaz2011} have helped investigators format their metadata to facilitate data analysis and data upload to repositories such as the \gls{ebiena}. Nevertheless, as samples are increasingly processed in parallel with multiple different protocols (i.e., 16S, 18S, \gls{its}, shotgun, metabolomics, etc.), correct formatting of metadata to capture this information and facilitate multi-omics correlative analyses will require careful attention and appropriate implementation of tools capable of handling hundreds to thousands of columns of data for hundreds to thousands of samples. Tools such as Qiita \footnote{\label{qiitaurl}\url{http://qiita.microbio.me}} are being developed to address the challenges associated with analyzing large numbers of samples, processed via multiple different protocols, and with complex metadata-and these tools rely on both the availability and effective usage of large-scale compute resources. The ability to apply tools such as \gls{qiime} in the cloud; e.g., using \gls{aws} \cite{Ragan-Kelley2013}, has broadened these capabilities far beyond the original user base, and enabled users in developing countries such as Bangladesh to use these tools without operating their own large-scale compute infrastructure. These techniques are now being applied in the United States through Illumina's BaseSpace \footnote{\label{basespaceurl}\url{https://basespace.illumina.com/home/index}} and NIH's Cloud Pilot \footnote{\label{cloudpiloturl}\url{https://commonfund.nih.gov/bd2k/commons}}.

\subsection{From bits to knowledge: how is big data moving microbiome research forward?}

Initial efforts to characterize and understand the healthy human microbiome using \gls{ngs} techniques \cite{TheHumanMicrobiomeProjectConsortium2012, Consortium2012} raised more questions than answers, and led to the explosion of microbiome research that has identified associations between the microbiome and diseases as varied as obesity, inflammatory bowel disease, cardiovascular disease, and autism (among many others). Most of these studies have simply identified associations and the question of causation or simple association remains unknown. Key studies, such as the obesity work done by Jeffrey I. Gordon and his team at Washington University \cite{Turnbaugh2006, Turnbaugh2009, Ridaura2013} and the personalized nutrition work done by Eran Segal of the Weizmann Institute \cite{Zeevi2017} are coming closer to answering the question of causality versus association. However, it is becoming increasingly clear that integrating DNA sequence data with other ‘omics techniques such as metatranscriptomics (sequencing the RNA), proteomics (sequencing the proteins), and metabolomics (characterizing the metabolites) will be key for advancing microbiome research. An example of the power of combining multiple techniques for assessing the microbiome is the \gls{nih} \gls{hmp}, the largest human microbiome sequencing effort at the time of its publication in 2012. 16S \gls{rrna} gene amplicons were generated from total of 4788 samples collected from 242 healthy adults \cite{TheHumanMicrobiomeProjectConsortium2012} and sequenced using 454 pyrosequencing. Additionally, a whole genome shotgun sequencing on the paired-end Illumina platform was performed on a subset of 681 samples, generating 2.9 Gigabases per sample (close to 2 terabytes of data for the entire dataset).

The \gls{hmp} shotgun metagenomics data revealed a key observation: while no taxon was observed in all individuals (i.e., no “core” healthy microbiome was identified), the functional pathways inferred from the shotgun data were evenly distributed across individuals and body sites. While this was an important observation, the addition of other data types, such as RNA-seq or metabolomics would have provided precise information regarding the actual activity of the microbial community and which small molecules were present, respectively, further exemplifying importance of combining different -omics techniques for generating hypotheses that ultimately lead to studies designed to obtain a more complete picture of a given microbial community (and the significance of its presence). For example, as reported by Bouslimani et al. \cite{Bouslimani2015}, using a paired sequencing-mass spectrometry approach allowed the investigators to identify correlations between Propionibacterium genera and the presence of oleic acid, palmitic acid, mono-oleic, and palmitic acylated glycerols on human skin. Hypothesizing that Propionibacterium mediates the hydrolyzation of triacylglycerides or diacylglycerides from human acylated glycerols, Bouslimani et al. cultured Propionibacterium acnes in a medium supplemented with the triglyceride triolein and examined the resulting metabolic products, ultimately confirming their hypothesis.

Microbiome citizen science initiatives such as the \gls{agp} \footnote{\label{agpurl}\url{http://americangut.org}} have made significant contributions to the field by “democratizing” microbiome research and thus providing large-scale datasets that can be used as comparative frameworks for other studies. Citizens support the science by sending samples from their bodies, their pets, or their environment as well as the necessary funds to cover the sample processing. These projects face the challenge of dealing with large numbers of samples; while most current microbiome studies contain hundreds or a few thousand samples, these citizen science efforts contain a continually growing number of samples that in some cases are on the order of over ten thousand samples, pushing the limits of the current computational tools. Furthermore, this democratization is not free: subject data is self-reported, and at times, significant amounts of data are necessary to correctly characterize the sample source. The \gls{agp} currently collects up to 400 variables about study participants, including detailed dietary information proffered through a standardized food frequency questionnaire (VioScreen). Analyzing all these variables is a challenge, and one solution is crowd sourcing the data analysis itself. All de-identified \gls{agp} data are made public as soon as they are available, allowing researchers and clinicians around the world to use the data to identify correlations between those variables and the microbiome data which can generate new hypotheses, or to contextualize their own studies with the largest open source human microbiome dataset that currently exists. The power of meta-analyses is apparent from early work by Lozupone and Knight \cite{Lozupone2007}, in which 21,752 16S \gls{rrna} sequences from diverse environments sampled across 111 studies were analyzed together to find that the main environmental driver differentiating microbial communities was salinity, rather than temperature, humidity, or a number of other environmental factors. However, when we restrict the analysis to the human gut microbiome, technical factors that differ between studies, such as DNA extraction, \gls{pcr} primers, and sequencing platform are often larger than the biological effects we seek to discover \cite{Lozupone2013}. Performing similar large-scale meta-analyses with the \gls{agp} data and the hundreds of other publicly available human microbiome datasets will be critical for identifying universal microbiome signatures associated with different health and disease states, and for understanding which technical variables have larger effect sizes than biological variables.
Big Data has also proven critical in the context of microbial epidemiology. Using Mycobacterium tuberculosis as an example, Guthrie and Gardy \cite{Guthrie2017} describe the utility of using \gls{ngs} techniques for understanding disease outbreaks. Whole genome sequencing of a specific pathogen can reveal the infection path (including patient 0) of the outbreak by allowing investigators to follow mutations from several strains isolated from infected individuals. Whole genome sequencing can also be used to diagnose disease. For example, determination of antibiotic resistance of M. tuberculosis is a notoriously difficult clinical problem; current gold-standard diagnostic techniques are culture-based and can take up to 8 weeks to generate results. Whole genome sequencing can reduce this time to a few days when the mutations responsible for drug resistance are well characterized and the reference databases are high quality. As a byproduct, the usage of whole genome sequencing for outbreak tracking and rapid diagnostics generates a genome catalogue that can be used for new drug development as well as better disease characterization. Clinical sequencing and diagnostic timeframes are becoming even faster with the advent of nanopore sequencing technology, currently commercialized by \gls{ont} through the MinION sequencer. The reads produced by \gls{ont} devices are longer but comparatively less accurate compared to other sequencing technologies; however, they are generated extremely rapidly and portably. Similar in size and price to a high-end smartphone, the MinION sequencer facilitates near-immediate data acquisition, meaning sequences can be generated much closer to the biological source. Nanopore sequencers have been used to perform same-day diagnosis of tuberculosis \cite{Votintseva2017} as well as in situ monitoring of an Ebola outbreak \cite{Quick2016}. The speed and portability can also benefit non-epidemiological microbiome work by making field-based work where sample transit and storage are difficult to impossible more obtainable. The MinION has already been used for on-site microbiological surveys in Antarctica \cite{Johnson2017} and produced the first sequences generated in space aboard the International Space Station \cite{Castro-Wallace2016}.

\subsection{Looking to the future: opportunities and challenges}

The tools and technologies that have enabled microbiome research thus far continue to improve at breakneck pace. Increased usage of fast, portable sequencers such as the MinION and of multi-omics techniques means that the amount of data collected by microbiome researchers will quickly reach never before seen sizes, which will pose challenges for data storage and analysis. This wealth of information also will facilitate the understanding of bacterial community mechanics and interactions like never before, leading to groundbreaking developments not only in human health \cite{VanNood2013, Cox2015, Ling2015}, but also in agriculture \cite{Sessitsch2015}, biofuels \cite{Hess2011}, and many other applications. One of the biggest challenges facing the field as investigators aim to achieve these goals is the ability to integrate and correlate the massive amounts of data produced by these protocols and to identify biologically relevant information that can be used to formulate testable hypotheses.
As investigators begin to utilize and combine multi-omics technologies, they are faced with tools and protocols that are at different stages of development. For example, one of the difficulties associated with mass spectrometry analysis of small molecules is that in many cases we are unable to determine whether molecules are microbial or host-derived due to lack of annotation, and if indeed derived from the microbiome, which specific group(s) of bacteria generated the chemical signature. Applying mass spectrometry techniques to more and more microbiome datasets will enable researchers to build the existing databases. Even among sequence data, biases exist towards well studied environments, such as the human gut, while less studied environments, such as coral reefs, are not represented accurately (Earth Microbiome Project, in review). Developing tools to cross-compare sequence and small molecule data is also a key challenge; many of the techniques to assess sequence data are phylogeny based and cannot be applied to mass spectrometry outputs. Additionally statistical approaches for assessing microbiome sequence data \cite{Mortone00162-16, Mandal2015} will need to be validated on mass spectrometry data, or new, appropriate tools will need to be developed. Finally, visualizing multi-omics data together in a clear, meaningful way poses an interesting challenge, particularly given that such tools will need to be able to process thousands of data points from thousands of samples.
Large-scale meta-analyses, such as those described in the previous section, also pose a unique challenge. Current 16S \gls{rrna} studies contain tens of millions of reads, and the amount of data utilized in meta-analyses is likely to be orders of magnitude larger as shotgun sequence and metabolomics data become a routine part of microbiome studies. The largest known meta-analysis in existence, performed on the first 27,742 samples from 91 different studies in the \gls{emp} \footnote{\label{empurl}\url{http://earthmicrobiome.org}} exposed key problems. First, the current tools utilized to analyze the data cannot handle more than 30,000 samples at a single time. Additionally, the importance of standardizing metadata also became crystal clear. Although standard metadata definitions exist, data repositories currently do not enforce their compliance, and the metadata normalization effort is shifted to the researcher performing the meta-analysis. New tools as well as more accurate documentation will be key to facilitate the adoption of the standards in the community.

Last but not least, one of the most important challenges that will face microbiome research in the near future is the translation of results from the laboratory to everyday life. The human body is a supra-organism containing a wide variety of microorganisms that provide up to 99\% of the genetic material present in our bodies. Ignoring this part of the system when assessing the well-being of a person is akin to performing a routine physical but only checking the blood pressure of the patient. Although the ultimate goal of human microbiome research is to implement clinical microbiome surveys, there is much work to be done before this goal can be realized. First, and most importantly, more data need to be collected and analyzed. Well-designed studies on clinical cohorts will be key for identifying meaningful host-microbiome associations and how these associations can be leveraged to improve human health. Universal \gls{sop} will also be critical to minimize lab to lab variation \cite{Sinha2015}, including protocols for sample collection, handling, storage and processing, as well as standardizing analysis tools. Clinician education will also be critical to enable health care providers to understand the limits of microbiome research as well as the advantages, and easy to understand microbiome analysis reports will be a key part of clinician education. Finally, sample processing and analysis times and costs need to be reduced. While in some cases genomic analysis is more rapid than gold standard diagnostics, in many cases, the processing time and costs outweigh the advantages of these techniques. For example, RNA-seq remains a lengthy, complex approach. The MinION may be useful for addressing this issue as it is able to directly accept RNA without the requirement for cDNA generation; however, widespread use of this tool will likely be closely tied to a reduction in the current error rate suffered by the system.

Microbiome research is currently on the precipice of producing orders of magnitude more data than ever before. To accurately assess and utilize this data, investigators will rely on the development of tools, pipelines, and \gls{sop}s able to effectively handle big data. Together, researchers, clinicians, and computer scientists are poised to revolutionize microbiome research and its applications in human health, agriculture, food science, and a number of other critical fields.
