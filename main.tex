%
%
% UCSD Doctoral Dissertation Template
% -----------------------------------
% http:\\ucsd-thesis.googlecode.com
%
%
% ----------------------------------------------------------------------
% WARNING:
%
%  This template has not endorced by OGS or any other official entity.
%  The official formatting guide can be obtained from OGS.
%  It can be found on the web here:
%  http://ogs.ucsd.edu/AcademicAffairs/Documents/Dissertations_Theses_Formatting_Manual.pdf
%
%  No guaranty is made that this LaTeX class conforms to the official UCSD guidelines.
%  Make sure that you check the final document against the Formatting Manual.
%
%  That being said, this class has been used successfully for publication of
%  doctoral theses.
%
%  The ucsd.cls class files are only valid for doctoral dissertations.
%
%
% ----------------------------------------------------------------------
% GETTING STARTED:
%
% Lots of information can be found on the project wiki:
% http://code.google.com/p/ucsd-thesis/wiki/GettingStarted
%
%
% To make a pdf from this template use the command:
%  pdflatex template
%
%
% To get started please read the comments in this template file
% and make changes as appropriate.
%
%
% ----------------------------------------------------------------------
%
% A thesis using this template and class file was last successfully
% submitted on 2009/03/19 (at least as far as I know).
%
% If you successfully submit a thesis with this package please let us
% know.
%
% ----------------------------------------------------------------------
% If you desire more control, please see the attached files:
%
%   * ucsd.cls    -- Class file
%   * uct10.clo   -- Configuration files for font sizes 10pt, 11pt, 12pt
%     uct11.clo
%     uct12.clo
%
% ----------------------------------------------------------------------



% Setup the documentclass
% default options: 11pt, oneside, final
%
% fonts: 10pt, 11pt, 12pt -- are valid for UCSD dissertations.
% sides: oneside, twoside -- note that two-sided theses are not accepted
%                            by OGS.
% mode: draft, final      -- draft mode switches to single spacing,
%                            removes hyperlinks, and places a black box
%                            at every overfull hbox (check these before
%                            submission).
% chapterheads            -- Include this if you want your chapters to read:
%                              Chapter 1
%                              Title of Chapter
%
%                            instead of
%
%                              1 Title of Chapter
\def\RELEASE{}
\ifdefined\RELEASE
    \documentclass[12pt,chapterheads]{ucsd}
\else
    \documentclass[12pt,chapterheads,draft]{ucsd}
\fi


% Include all packages you need here.
% Some standard options are suggested below.
%
% See the project wiki for information on how to use
% these packages. Other useful packages are also listed there.
%
%   http://code.google.com/p/ucsd-thesis/wiki/GettingStarted



%% AMS PACKAGES - Chances are you will want some or all
%    of these if writing a dissertation that includes equations.
%  \usepackage{amsmath, amscd, amssymb, amsthm}

%% GRAPHICX - This is the standard package for
%    including graphics for latex/pdflatex.
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}{>{\centering\arraybackslash} m{2cm} }

\usepackage{hyphenat} % allow for hyphenated words to be further hyphenated
                      % https://tex.stackexchange.com/a/2715
\usepackage{rotating}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{url}

%% SUBFIGURE - Use this to place multiple images in a
%    single figure.  Subfigure will handle placement and
%    proper captioning (e.g. Figure 1.2(a))
% \usepackage{subfigure}

%% LATIN MODERN FONTS (replacements for Computer Modern)
% \usepackage{lmodern}
% \usepackage[T1]{fontenc}


%% INDEX
%   Uncomment the following two lines to create an index:
\usepackage{makeidx}
\makeindex
%   You will need to uncomment the \printindex line near the
%   bibliography to display the index.  Use the command
% \index{keyword}
%   within the text to create an entry in the index for keyword.

%% HYPERLINKS
%   To create a PDF with hyperlinks, you need to include the hyperref package.
%   THIS HAS TO BE THE LAST PACKAGE INCLUDED!
%   Note that the options plainpages=false and pdfpagelabels exist
%   to fix indexing associated with having both (ii) and (2) as pages.
%   Also, all links must be black according to OGS.
%   See: http://www.tex.ac.uk/cgi-bin/texfaq2html?label=hyperdupdest
%   Note: This may not work correctly with all DVI viewers (i.e. Yap breaks).
%   NOTE: hyperref will NOT work in draft mode, as noted above.
 \usepackage[colorlinks=true, pdfstartview=FitV,
             linkcolor=black, citecolor=black,
             urlcolor=black, plainpages=false,
             pdfpagelabels]{hyperref}
% \hypersetup{ pdfauthor = {Your Name Here},
%              pdftitle = {The Title of The Dissertation},
%              pdfkeywords = {Keywords for Searching},
%              pdfcreator = {pdfLaTeX with hyperref package},
%              pdfproducer = {pdfLaTeX} }


\usepackage{verbatim}

\usepackage[toc,nopostdot,style=alttree]{glossaries}
\glssetwidest{PERMANOVAX}% widest name
\makeglossaries
\include{abbreviations}

\begin{document}


%% FRONT MATTER
%
%  All of the front matter.
%  This includes the title, degree, dedication, vita, abstract, etc..
\include{frontmatter}


%% DISSERTATION

\chapter{What is the microbiome and why is it important?}\label{chapter_overview}
\glsresetall

The microbiome is the group of microorganisms that live in, on, and around us.
Understanding the interactions between these microorganisms and their niche can
lead to advances in a wide variety of research areas, including, but not limited
to human and pet health \cite{Cox2015, Gevers2014, Vazquez-Baeza2016}, forensics
\cite{Fierer2010, Hyde2017}, climate change \cite{Tas2014} and pharmaceuticals
\cite{Novo2000, Maurice2013}.

Researchers have at their disposal multiple technologies that allow them to make
different inquiries about the microbial system that they are investigating.
Target gene sequencing (such as 16S/18S \gls{rrna} or \gls{its} marker gene
sequencing surveys) allows researchers to survey the different bacteria and
Archaea present in a sample. Shotgun metagenomics provides a view of the functional
potential of the microbial community, and metatranscriptomics confirms gene
expression at a specific time point. Metabolomics surveys provide a view of
the small molecules contained in a sample, either from microbial or host origin.
The combination of the results of these technologies is crucial to establish
correlations between the microorganisms and their environment. The vast amount
of data generated by these technologies requires fast and efficient resources
and tools for effective data analysis. The work presented in this thesis is
motivated by the challenges that microbial ecologists are facing when analyzing
such large microbiome datasets, some of which are summarized in the following
section, which appeared in the journal \textsl{Current Opinion in Systems Biology, 2017}.

% \include{chapter_overview}
\section{The microbiome and big data}\label{section_bigdata}
Add paper: ``The microbiome and big data''. J. A. Navas-Molina, E. R. Hyde, J. G. Sanders and R. Knight. \emph{Current Opinion in Systems Biology}, 2017, DOI: 10.1016/j.coisb.2017.07.003.

\chapter{Analyzing large scale microbial community cohorts}\label{chapter_book}
\glsresetall

Chapter~\ref{chapter_overview} exposed some of the challenges that researchers
face when analyzing the big datasets resulting from microbial community
studies. These challenges are exacerbated in massive iniatives like
the \gls{emp} \cite{Gilbert2010, Gilbert2014, Thompson2017} and the \gls{agp}
\footnote{\label{agpurl}\url{http://americangut.org}}.

The \gls{emp} is a collaborative effort of more than 500 investigators, and aiming
to characterize the Earth's microbial life using amplicon sequencing,
metagenomics and metabolomics. The goal of the \gls{emp} is to process up to 200,000
samples for 16S \gls{rrna} marker gene sequencing, releasing the data as the data are
generated prior to publication and
crowd sourcing the data analysis. More than 60 publications have already resulted
from data generated by the \gls{emp}, and these average 17 citations per paper per year
\footnote{\label{emppuburl}\url{http://www.earthmicrobiome.org/publications/}}.
Currently, the \gls{emp} is in its second phase, in which the researchers aim to
generate the shotgun metagenomic assemblies and metabolomic profiles of 500 samples.

The \gls{agp} is the largest crowd sourced, crowd funded, citizen science project. It aims to
open microbiome analysis to anyone, and to create a vast, free, microbiome dataset.
In the \gls{agp}, the participants donate a sample (typically a stool sample,
although the project is not limited to stool) as well as the financial contribution
that supports processing, sequencing and analysis of the sample. \gls{agp} participants complete a
questionnaire with dietary and lifestyle questions. The answers to the
questionnaire are then stored in a standard set of columns, and are
made publicly available together with the sequences, both de-identified.

Both of these initiatives are releasing a powerful framework to the research
community. On the one hand, researchers around the globe can mine the datasets to
find new trends and generate new hypotheses. On the other hand, they can use
it to contextualize their own samples and increase the power of their own analyses
by comparing them against these cohorts. The success of these type of initiatives,
however, relies on the use of standardized practices for sample processing and
analysis, to minimize technical differences between the samples. The \gls{emp}
released standard protocols \footnote{ \url{http://www.earthmicrobiome.org/protocols-and-standards/}}
for sample collection and preparation, which have been widely adopted by the community and cited
around 2000 times \textsuperscript{\ref{emppuburl}}. Normalizing the data
analysis is as important as normalizing the data processing to minimize
technical differences. %TODO: Cite Lin 2007 and 2008 - Ask Rob for more info on those citations
\gls{qiime} \cite{Caporaso2010}, an open-source pipeline for analyzing microbiome
data developed in the Knight lab, is one of the most popular \footnote{The original paper
has been cited over 8,700 times according to Google Scholar at the moment of this writing}
tools for those analyses. With 155 scripts and over 1000
parameters, the risk of introducing technical differences during the data analysis
step is high. Section~\ref{section_book}, published in the journal
\textsl{Methods in Enzymology, 2013}, contains the first gold standard approach
for the analysis of microbial community datasets, as well as providing to the
researchers with suggestions about how to minimize the introduction of
technical differences while analyzing the data. As the first author of this
publication, I co-wrote the text, generated the majority of the figures and wrote
the IPython notebook \cite{Perez2007} attached to the publication.


% \include{chapter_book}
\section{Advancing our understanding of the human microbiome using QIIME}\label{section_book}
Add paper: ``Advancing our understanding of the human microbiome using QIIME''. J. A. Navas-Molina, et al. \emph{Methods in Enzymology}, 2013, DOI: 10.1016/B978-0-12-407863-5.00019-8.

\include{chapter_otupicking}
\include{chapter_contributions}

\chapter{Better memory management in the cloud}\label{chapter_cudswap}
\glsresetall

Chapter~\ref{chapter_book} described the first gold standard for analyzing microbial
community datasets, exposed and alleviated some of the computational bottlenecks in the
pipeline and showed some examples of the application of such pipeline. Some of the
steps of the presented pipeline are too computationally expensive to be run in a
personal laptop, and access to a supercomputer is needed to complete those steps.
However, microbiologists do not necessarily have access to supercomputers and they have
to rely on cloud computing. Tools such \gls{qiime} \cite{Caporaso2010} and IPython
\cite{Perez2007} provide \gls{ami}s that enable biologists to run their analysis in
Amazon's \gls{ec2} infrastructure \cite{RaganKelley2013}. This facilitates microbial
biologists' work by avoiding the often complex task of installing the required software to run
their analyses as well as providing an environment suitable to support the large
datasets that then currently face. But running these analyses in the cloud presents
new challenges to microbiologists. One of the first decisions that microbial
biologists face when running these analyses in the cloud is choosing the resources
that their virtual machine in the cloud should contain. Usually microbiologists
are unaware of the resources required by the analysis tools that they are going to
use, and often the requirements of these tools is highly dependant on the nature
of the data. In these cases, the biologists are left with a trial and error procedure
until they have enough resources for their analysis or they have to choose a virtual
machine with more resources than needed that they go underutilized. In both of these
cases, the microbiologists end up spending more money (and time) than
needed, which can be critical for those scientists running on a budget.

Sections~\ref{section_memory_exhaustion} and~\ref{section_cudswap} expose that
the most critical resource on Amazon's \gls{ec2} is memory, and they describe and
implement a solution that mitigates the impact of the trial and error procedure, by
allowing the user's task to be completed at a small expense on running time. The material
in sections~\ref{section_memory_exhaustion} and~\ref{section_cudswap} was published
in the \textsl{43rd Annual IEEE/IFIP International Conference on Dependable Systems
and Networks, 2013} and the \textsl{International Conference on Cloud and Autonomic
Computing, 2014}, respectivelly. As the first author of these publications, I
conceived the idea, designed and implemented the software, designed and executed
the benchmarks and wrote the text.

% \include{chapter_memory_exhaustion}
% \include{chapter_cudswap}
\section{Addressing memory exhaustion failures in Virtual Machines in a cloud environment}\label{section_memory_exhaustion}
Add paper: ``Addressing memory exhaustion failures in Virtual Machines in a cloud environment''. J. A. Navas-Molina, S. Mishra. \emph{43rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks}, 2013, DOI: 10.1109/DSN.2013.6575330.
\section{CUDSwap: Tolerating Memory exhaustion failures in cloud computing}\label{section_cudswap}
Add paper: ``CUDSwap: Tolerating Memory exhaustion failures in cloud computing''. J. A. Navas-Molina, S. Mishra. \emph{International Conference on Cloud and Autonomic Computing}, 2014, DOI: 10.1109/ICCAC.2014.12.

\chapter{Meta-analyses: importance, challenges and solutions}\label{chapter_qiita}
\glsresetall
A unifying theme in the work presented in section~\ref{section_contributions}
is that it takes advantage of previously published datasets to increase the power
of the findings. The Komodo Dragon paper \cite{Hyde2016} compared new findings
on captive Komodo dragons with wild amphibians \cite{Kueneman2014} and humans and
pets living in homes \cite{Lax2014} to hypothesize that the lack of interactions
with an open environment can negatively affect human and animal health. The \gls{emp}
\cite{Gilbert2010, Gilbert2014, Thompson2017} combined samples from 97 independent
studies to answer spatial, temporal and evolutionary microbial community questions at a global scale.
The \gls{agp} used previous studies to show that the findings resulted from citizen-science
microbiome research can replicate previous results and, move research forward
by creating a massive dataset of human samples that can be used to
generate new hypotheses. Finally, the manuscript about correcting microbial blooms
\cite{Amir2017Bloom} used previously published datasets to describe a new technique
used to reduce technical differences caused by sample shipping at room temperature.

This technique of using multiple datasets (published or not)
to improve the findings of a study is known as a meta-analyses. As shown
by the previous examples, meta-analyses is a powerful tool that is becoming increasingly
common on microbiome research \cite{Lozupone2007, Ley2008, Sinha2017}. However,
this extra power comes with its own set of challenges that can delay the publication
of the results from months (as in the Komodo Draogn manuscript) to years (as in
the \gls{emp} manuscript). Namely, this challenges can be grouped in three topics:
(1) technical differences, (2) data availability, and (3) data standardization.

Technical differences are a result of differences on handling the samples, and
they can originate in any step of the proces: from decisions about sample collection and
preservation \cite{Song2016}, the \gls{pcr} primers or sequencing platform of choice
\cite{Kuczynski2011, Tremblay2015}, or even the laboratory in which the samples
are processed \cite{Sinha2017}. These technical differences can overpower
the underlying biological differences, making meta-analyses almost impossible. Thus,
it is critical that this information is captured and made available at publication
time, so other researchers can reproduce the results and/or decide if they can use
the published data in a meta-analyses with their own samples.

The second challenge presented to a researcher wanting to perform a meta-analysis
is collecting the data of the previously published results. Although current publishers
usually require the data to be deposited in a long-term repository such as
the \gls{ebiena}, there are no mechanisms that ensure that the data deposited is valid
or complete. Besides the DNA sequence data, the researcher must also track down
the metadata describing the samples. Even when the metadata publicly available,
it may be incomplete and/or the specific encoding may not
be clear for other users, requiring communication with the original authors to
decode the information.

Finally, the researcher wanting to perform meta-analyses needs to combine and normalize
his/her data with the previously published data to perform such meta-analyses.
Although standards exist to represent sample metadata, such as the \gls{mimarks} standard
\cite{Yilmaz2011}, they are not enforced by the long-term repositories or even required.
This process of normalizing metadata is tedious and hard to automate, increasing
the risk of introducing errors in the metadata which can alter the results
of the meta-analyses. Apart from the metadata, the DNA sequence data itself may
not be normalized. Sequence data available in the long-term repositories is not ensured
to be the raw data. Preprocessing performed on those
sequences, such as quality control, can introduce technical differences that affect
the results of the meta-analyses and reduce ability to find biological differences.

Section~\ref{section_qiita} presents Qiita, a web-based service designed to
alleviate the meta-analyses challenges by enforcing standards, requiring sample
handling information, normalizing raw data representation and processing and hosting
over 150,000 samples publicly available from around the world. The material in section~\ref{section_qiita}
is submitted for publication in \textsl{Nature Methods}. As a first
co-author of this publication, I have been involved in the design and implementation of
the database, graphical user interface and plugin system of Qiita and wrote the text.

\section{Qiita accelerates microbiome meta-analyses from months to minutes}\label{section_qiita}
Add paper: Qiita paper (in prep).

\chapter{Making meta-analysis accessible to the clinician}\label{chapter_rapid_response}
\glsresetall

Section~\ref{section_bigdata} introduced one of the most important challenges in
microbiome research: translating the research results from the laboratory to
everyday life, in particular to human health. The human body is a complex ecosystem
hosting a wide variety of microorganisms that play key roles in our well-being.
However, this aspect of the human body is generally ignored during routine
doctor's visits.

One of the reasons why microbiome analyses are not routinely used in the clinic
is because microbiome research is still in its infancy and more well-designed
studies on clinical cohorts are needed to identify microbiome-host interactions
that can directly be applied to human helth. Reproducibility of these results,
integration of multiple datasets and standardization of the data is key to achieve
the quality results needed for clinical applications. Sections~\ref{section_book}
and~\ref{chapter_qiita} presented improvements to these challenges.

Another challenge for bringing microbiome analysis to the clinic is the time that
it takes to perform a microbial community analysis. Typical microbiome
analyses can take from months to years to complete. However, the time to
perform these analyses can be reduced by following \gls{sop} for sample handling
and processing, using an analysis standardization framework, optimizing analysis
bottlenecks and employing an interdisciplinary team of experts to analyze the data.
Section~\ref{section_48hours} shows how, using the work presented so far in this
dissertation and the interaction of an interdisciplinary team of analysis experts,
microbial community analyses can be performed in under 48 hours, a time frame short
enough to provide useful information in a clinical setting.

The analysis effort described in section~\ref{section_48hours} provided enough
information to identify additonal bottlenecks that could be alleviated to further
reduce analysis time. Specifically, one of the bottlenecks was the transfer of sample
processing information from the wet lab to the dry lab, as well as an efficient
method to record said information, which is critical to assess the quality of the sequencing run.
Section~\ref{section_platemapper} presents a system built on top of Qiita that
provides an easy way to record all the wet lab information and easily transfer
it to the dry lab. Furthermore, by systematically storing the information in a
structured way, human interaction can be reduced in multiple parts of the pipeline,
so analyses can be run automatically as soon as the sequences are available.
This generates an initial overview of the data as fast as possible, providing
the data analyst with enough information to decide how to move forward with a
more in-depth analysis.

The material in section~\ref{section_48hours} was published in \textsl{mSystems, 2013}.
As a co-first author of this publication, I performed the fast initial 16S analysis,
generated the starting point data for more in-depth analyses of the 16S data,
directed the 16S analysis team, generated figures for the publication, and wrote the text.

% The material in section~\ref{section_platemapper} is submitted for publication in
% \textsl{TODO}. As a first author of this publication, I have been involved in the
% design and implementation of the overall PlateMapper system and wrote the text.

\section{From sample to multi-omics conclusions in under 48 hours}\label{section_48hours}
Add paper: ``From sample to multi-omics conclusions in under 48 hours''. R. A. Quinn, J. A. Navas-Molina, E. R. Hyde, S. J. Song, Y. V\'azquez-Baeza, G. Humphrey, J. Gaffney, J.J. Minich, A. V. Melnik, J. Herschend, J. DeReus, A. Durant, R. J. Dutton, M. Khosroheidari, C. Green, R. da Silva, P. C. Dorrestein, R. Knight. \emph{mSystems}, 2016, DOI: 10.1128/mSystems.00038-16.

\section{PlateMapper}\label{section_platemapper}

\chapter{Conclusions}\label{chapter_conclusions}
\glsresetall
% \include{chapter_conclusions}

\appendix
\include{appendix}

%% END MATTER
\printindex %% Uncomment to display the index
% \nocite{}  %% Put any references that you want to include in the bib
%               but haven't cited in the braces.
% \bibliographystyle{alpha}  %% This is just my personal favorite style.
%                              There are many others.
\bibliography{references}
\bibliographystyle{plain}

\end{document}
